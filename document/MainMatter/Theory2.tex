\chapter{Marco te'orico y preliminares}\label{chapter:background}

% Introducción a Deep Learning, Neural Networks: Tipos de aprendizaje, esquema de minimización.
% Explicaciones de los modelos usados: Funciones de activacion, Relu, softmax
% Explicaciones de los modelos usados: Funciones de error, categorical crossentropy
% Explicaciones de los modelos usados: LSTM y Bidireccional, CRF, CNN, ResNet, DenseLayer, GloVe, Attention
% Explicaciones de los modelos usados: Optimizadores Adam, Stocastic Gradient Descent, Diferenciacion automática

En este cap'itulo se introducen conceptos de aprendizaje automático. Temas
como la representación de datos, arquitecturas, evaluación de modelos son tratados en sus secciones.
Se menciona las diferentes investigaciones realizadas en la EA a lo largo de los annos, viendo
los enfoques tomados y la evoluci'on de estos a partir del desarrollo e introducci'on de nuevos m'etodos en el
campo. Por 'ultimo se introduce la proyección de corpus, t'ecnica necesaria para la creación de conjuntos 
de datos necesarios para el entrenamiento de modelos de aprendizaje automático.

\section{Aprendizaje Automático}

Existen varios problemas para los cuales es difícil escribir un programa de forma tradicional que pueda
resolverlo, como decir si en una imagen existe un gato o un perro, o transcribir una grabación. En el caso
de que se escribiera este programa probablemente sería frágil y poco escalable. El aprendizaje automático
(AA) constituye el estudio de técnicas que puedan 
aprender de la experiencia [\cite{d2l}]. Con esto se puede automatizar el proceso de encontrar soluciones a 
dichos problemas, haciendo que sus resultados sean robustos y escalables. En AA existen tres tipos de 
aprendizaje, el aprendizaje no supervisado TODO ESCRIBIR. Otro tipo de aprendizaje es el reforzado el cual 
TODO ESCRIBIR. Finalmente se encuentra el aprendizaje supervisado TODO ESCRIBIR. A este útlimo principalmente 
se refiere la sección.

Los componentes básicos de un problema de aprendizaje supervisado se pueden resumir en los datos de los que 
aprender, el modelo para transformar los datos, una función de pérdida que cuantifica qué tan malo o bueno es el 
modelo y un algoritmo que ajuste los parámetros del modelo para minimizar la pérdida.

Matemáticamente un modelo constituye una función $\hat{y} = f_{\theta}(x)$ a la cual se 
le pasan representaciones de las entradas orginales $x$ y devuelve una salida $\hat{y}$. Esta función $f$ se encuentra
parametrizada por el vector $\theta$ el cual constituyen los parámetros en los que el modelo se basa para realizar sus
cálculos. La función de pérdida se puede definir como $e(y, \hat{y})$. El objetivo final del algoritmo de ajuste 
se define como encontrar $\theta$ tal que (\ref{eq:arg_min_theta}):

\begin{equation}
	\arg \min_{\theta} e(Y, f_{\theta}(X))
\end{equation}\label{eq:arg_min_theta}

Los parámetros $\theta$ eventualmente se van ajustando con algún algoritmo de optimización para que alcance 
un valor óptimo, aunque un mínimo global en la práctica es dificil de conseguir.

Una manera de observar la complejidad de estos modelos sería la cantidad de capas de procesamiento que lo integran.
En los principios se empezaron utilizando modelos de una sola capa\footnote{A estos modelos se les conoce 
como \emph{shallow} o poco profundos} para realizar 
las tareas, más tarde estos modelos fueron ganando en complejidad al superponerle otras dando paso al 
aprendizaje profundo (\textbf{DL} por sus siglas en inglés \emph{Deep Learning}). La superposición se refiere
a la composición de funciones $f_i$ y $f_{i+1}$ de tal forma que la imagen de $f_i$ sea compatible con el dominio de 
$f_{i+1}$ entonces se obtiene $f_{i+2}(x) = f_{i+1}(f_i(x))$, al aplicar este esquema se pueden agregar múltiples
capas y profundizar $f$ tanto como se desee.


\subsection{Representación de datos}

La representación de los datos constituye una parte importante en modelar un problema. Esta 
es la encargada de presentar datos abstractos, como imágenes, párrafos, sonidos, en formas tratables
por los algoritmos. Generalmente se buscan configuraciones que recojan la mayor cantidad de información 
de la entrada relevante al problema.

En PLN la representación suele estar representada a distintos niveles de granulidad.
De menor granulidad a mayor se pueden ir mencionando los documentos, párrafos, oraciones, palabras y
caracteres. De estos elementos es posible representarlo mediante vectores que codifiquen propiedades
objetivas del problema a tratar como características morfológicas o semánticas de estos.

Las características morfológicas son aquellas que describen cómo está formado el elemento a analizar.
Estas pueden ser extraídas con relativa facilidad, entre tales características se encuentran tamaño, 
cantidad, posición de palabras o párrafos, presencia de sufijos, prefijos, acentos u otros marcadores
en el texto. Las características semánticas presentan una mayor dificultad a la hora de ser extraídas.
Para esto se utilizan diferentes modelos que codifican esta información en vectores conocidos como 
\emph{embeddings}. Para esto se existen varios modelos 
como \textbf{word2vec} [TODO ref], \textbf{GloVe} (TODO Poner que signica las siglas y Referencia), \textbf{BERT}
(TODO Poner que signica las siglas y Referencia).
En \textbf{word2vec} modelan ... (TODO continuar), \textbf{GloVe}
sigue el enfoque de ... (TODO continuar). \textbf{BERT} se basa en ... (TODO continuar, poner las variantes RoBERTa, etc).

\subsection{Modelaci'on de problemas}

TODO Empezar con intro
TODO Hablar de problemas de clasificación
TODO Hablar de los modelos seq2seq, Hablar del etiquetado BIO y BIOES, hablar de agregar meta etiquetas a este 
tipo de problemas.


\subsection{Arquitecturas}

En aprendizaje profundo existen una gran cantidad de arquitecturas que se pueden utilizar para formar el modelo 
final.

\subsubsection{Capas densas}

El perceptron consiste es una transformación lineal del vector de datos $\textbf{x}$ con un bias $b$ y 
luego aplicar una transformación no lineal $g$, conocida como función de activación, 
para la obtención del resultado final:

\begin{equation}
	f(\textbf{x}) = g(\textbf{w}\textbf{x} + b)
\end{equation}\label{eq:perceptron}.

El perceptron constituye la unidad básica de las capas densas, ya que estas consisten en la aplicación
de este modelo varias veces sobre la misma entrada $\textbf{x}$ produciendo vectores de la dimensión $k$ 
deseada como salida final. Los parámetros se codifican en la matriz $W$ y sus bias en el vactor $b$:

\begin{equation}
	f(\textbf{x}) = g(\textbf{Wx} + \textbf{b})
\end{equation}\label{eq:dense}.

Para las funciones de activación existen varias elecciones. Una de estas es la función sigmoide. 
Esta devuelve un valor entre 0 y 1, es usada en tareas de regresión logística. 
Se puede interpretar como el nivel de activación de la neurona:

\begin{equation}
	sigm(x) = \frac{1}{1-e^{-x}}
\end{equation}\label{eq:sigmoide}.

\emph{Rectified Linear Unit} (ReLU) es definida como la parte positiva del argumento. Una ventaja que trae esta 
función de activación es su rápido cálculo de su derivada y que previene en parte de los problemas 
de desaparición de gradiente de la sigmoide:

\begin{equation}
	relu(x) = \max(0, x)
\end{equation}\label{eq:relu}.

La función de activación softmax es diferente a las anteriores en el sentido que necesita
el vector salida de la capa para ser computada. Esta función convierte las $q$ salidas
de la en una distribución de probabilidad, por esto es utilizada en tareas de clasificación:

\begin{equation}
	softmax_k(\textbf{x}) = \frac{e^{x_k}}{\sum_{i=1}^q x_i}
\end{equation}\label{eq:softmax}.

\subsubsection{Redes Neuronales Convolucionales}

Las redes neuronales convolucionales (\textbf{CNN} por sus siglas en inglés \emph{Convoutional Neural Networks}) 
son un tipo de redes usadas
principalmente en tratar datos en los que su estructura espacial es relevante, por ejemplo
en datos bidimensionales como imágenes y en unidimensionales como sonido y texto.
Estas redes aplican de una función de kernel sobre los datos, $f * g$ donde $f$ son los
datos, $g$ es el kernel o filtro y $*$ es el operador de convolución. La función $g$ se puede 
aprender en el proceso o también puede ser una función de agrupación predefinida. 
Un ejemplo bidimensional de una función de kernel constituye el siguiente caso:

TODO PONER UNA FOTO DE CONVOLUCION DE KERNEL

En este se observa cómo una nueva representación es computada al correr el kernel por la matriz de datos. Este
corrimiento se puede realizar de diferentes formas, por ejemplo se puede mover de dos en dos en vez de uno en uno, este
parámetro se le conoce como tamaño de paso (\emph{stride} en inglés). Es posible además preservar las dimensiones
iniciales de los datos al aplicarle un aumento de los datos en los bordes de tal forma que el resultado sea de la misma
dimensión, este aumento es generalmente rellenando convenientemente con 0 los espacios.

TODO PONER UNA FOTO DE CONVOLUCION DE KERNEL CON PADDING Y STRIDE DIFERENTE

Existen varias funciones de agrupación usadas. Entre estas se encuentran las de agrupación máxima, de 
agrupación media. Como sus nombres indican, la de agrupación máxima devuelve el valor máximo de los encontrados
en la ventana del kernel, la de media calcula el promedio de estos valores. Estas capas tienen la capacidad de obtener
información resumida sobre los datos.

\subsubsection{Redes Residuales}

Al crear modelos de aprendizaje profundo se tienen un conjunto de parámetros, las posibles combinaciones 
de estos forman un espacio de funciones $F$ al cual pertenencen todas las posibles instancias del modelo.
Agregar nuevas capas aumenta la complejidad de este, pero no hay garantía de que el viejo espacio 
de funciones $F$ sea subconjunto del nuevo espacio $F'$, lo que implica que el nuevo modelo no es necesariamente
estrictamente superior al antiguo. Este problema es la razón de la aparición de las Redes Residuales. 
Una red residual está formada por 
uno o varios bloques residuales, en los que a la salida de cada bloque residual le es sumada la entrada de 
este mediante una conexión residual.
El objetivo de realizar tal operación es que es posible hacer la contribución del bloque 0 obteniendo así
un modelo equivalente a uno sin el bloque, garantizando la condición de subconjunto $F \subset F'$, además 
dicho bloque no pierde poder expresivo dado que en caso de que su aporte sea considerable al resultado final, 
se tendría que aprender solamente la función $f(x) - x$ donde $x$ es la entrada del bloque para mitigar el 
efecto de la conexión residual y $f$ es la función aprendida por el bloque sin la conexión residual.

TODO FOTO DE RESNET. d2l pag 289

\subsubsection{Redes Neuronales Recurrentes}

Las redes neuronales recurrentes (\textbf{RNN} por sus siglas en inglés \emph{Recurrent Neural Networks}) son
un tipo especial de arquitectura especializada en el trabajo con datos secuenciales. Este tipo de arquitecturas
presentan variables en las que se almacenan información pasada que es usada para el computo de la salida. El 
problema se puede modelar mediante probabilidades mediante la estimación de $P(x_t | x_{t-1}, \dots x_{1})$,
donde existen dos variantes principales, se fija un tamaño de ventana en el tiempo $\alpha$ dando como resultado
$P(x_t | x_{t-1}, \dots, x_{t-\alpha})$ a este tipo de modelos se les conoce como autoregresivos. Otra estrategia
consiste en guardar un contexto de observaciones pasadas $h_t$ y con este realizar la estimación $P(x_t | h_t)$,
el contexto se actualiza en cada paso mediante una función $h_t = g(h_{t-1}, x_{t-1})$, a estos se les nombra
modelos autorregresivos latentes. 

Estos modelos presentan problemas de gradientes ya que estas pueden volverse extremadamente grandes o desaparecer.
Para esto se han creado arquitecturas que disminuyen estos problemas. Una de estas arquitecturas es las memorias
de corto largo plazo (\textbf{LSTM} for sus siglas en inglés \emph{Long Short Term Memory}) [\cite{TODO lstm}].
Este modelo guarda un contexto del procesamiento y está constituído por varias compuertas que regulan las 
actualizaciones de los estados internos. \textbf{LSTM} posee dos variables de estado, la memoria $C$ y el estado 
oculto $H$. Entre sus compuertas se encuentran la compuerta de olvido, esta regula cuanto de la memoria permanece
en el proximo paso, la compuerta de entrada ajusta la cantidad de información nueva que estrará, la compuerta 
de salida maneja el calculo del próximo estado oculto.

TODO FOTO DE LSTM d2l pag 357

El método de aprendizaje de las \textbf{RNN} solamente observa los elementos anteriores de la secuencia, aunque existen
tareas en las que observando los elementos posteriores brinda más contexto e información a la tarea sin que interfiera
en el proceso de inferencia. El modelo bidireccional presenta una alternativa para tratar estas tareas, este modelo
consiste en además de hacer la corrida de inicio a final de la secuencia, se realiza un desde al final hasta el  
inicio de esta, estas corridas van generando dos estados ocultos $\overrightarrow{H}_{i}$ y $\overleftarrow{H}_{i}$
que luego son mezclados para obtener el contexto final $H_i$.

TODO FOTO DE BIDIRECTIONAL LSTM d2l pag 367

\subsubsection{Atención}

% Definicion, qué hace, uso en secuencias
La atención es una técnica en la cual se hace una selección ponderada de atributos en un contexto específico. 
Este mecanismo presenta dos partes, una consulta $q$ y una collección de pares llave-valor $(k_i, v_i)$, la 
consulta representa el contexto en donde se quiere aplicar la atención y las llaves $k_i$ son elementos que 
relacionan la consulta a los valores $v_i$. El proceso de calcular el resultado consiste en primero calcular 
el vector compatibilidad $e$ entre las llaves y la consulta mediante la función $f$, este vector es luego 
modificado por una función $g$ que distribuye los valores obteniendo el vector atención $a$, finalmente 
este vector es utilizado para calcular el resultado final al aplicarle la función $o = z(a, V)$.

\begin{equation}
	e = f(q, K)
\end{equation}
\begin{equation}
	a = g(e)
\end{equation}
\begin{equation}
	o = z(a, V)
\end{equation}

En dependencia de cómo se seleccionen las funciones $f$, $g$ y $z$ se pueden obtener distintos tipos de atención.
Una configuración simple consiste en ser $f$ definida como el producto punto de la consulta con la llave,
$g$ softmax y $z$ la suma ponderada de $v_i$ con los valores de atención. 

\subsubsection{Campo Aleatorio Condicional}

% Definicion de CRF, qué modela, Ventajas de CRF en secuencias, mirar relacion con Hidden Markov Models

El campo aleatorio condicional (\textbf{CRF} por sus siglas en inglés \emph{Conditional Random Field}) es un 
tipo de modelo gráfico probabilístico con el objetivo de modelar eficientemente el trabajo con secuencias 
modelando conjuntamente la probabilidad de las etiquetas de las secuencias dada sus observaciones [\cite{lafferty2001conditional}].
En trabajos de secuencias la forma más simple que toma el grafo consiste en una cadena de las variables representando
las etiquetas de las secuencias $Y$ conectadas de la forma $(Y_i, Y_{i+1})$ y las variables observadas $X$ conectadas
a las variables $Y$ [\cite{wallach2004conditional}].

El objetivo de \textbf{CRF} es calcular la secuencia $Y^*$ tal que:

\begin{equation}
	Y^* = \arg \max_Y P(Y | X)
\end{equation}\label{eq:crf}.

En este se observa que devuelve la secuencia más probable dado las variables observadas o atributos $X$,
por lo que esta capa es usada al final para problemas de clasificación de secuencias.

\subsection{Evaluación del modelo}

En AA los modelos necesitan maneras de expresar qué tan buenos son 
en las tareas encomendadas. Para esto se crean funciones que evalúan los resultados obtenidos
por dichos modelos, estas funciones se les da el nombre de métricas. Existen diferentes tipos de
métricas para tratar con diferentes tipos de problemas. En aprendizaje supervisado una métrica se
define como una función $m_s(Y, \hat{Y})$ donde $Y$ son las predicciones verdaderas y $\hat{Y}$ son las predicciones
hechas por el modelo. En algoritmos de aprendizaje no supervisado como K-Means, K-NN son usadas funciones $m_{ns}(\hat{Y})$
donde $\hat{Y}$ son las predicciones finales. En comparación con su versión supervisada estas funciones no tiene acceso
a las predicciones verdaderas del problema.

\subsubsection{Clasificación}

En problemas de clasificación son empleadas medidas que toman en cuenta la naturaleza discreta de su conjunto im'agen.
Medidas como precisión, recobrado, certeza (\emph{accuracy} en inglés) y F1 son utilizadas principalmente en la 
evaluación de los resultados, mientras que como función de error se usa entropía cruzada comúnmente 
(\emph{cross entropy} en inglés) [\cite{grandini2020metrics}].

La matriz de confusión es una via de representar los resultados de dos clasificadores. Esta matriz en $M_{ij}$ 
indica la cantidad de elementos que clasificó como clase $i$ el primer clasificador y
como clase $j$ el segundo clasificador. En su uso práctico
un clasificador son las etiquetas verdaderas mientras que el otro es el clasificador que se está evaluando. 
En problemas de la clasificación binaria, donde se busca saber si existe pertenencia o no de un elemento a una clase,
se pueden observar los siguientes casos.

\begin{itemize}
	\item Verdaderos Positivos (VP): Elementos clasificados correctamente que pertenecen a la clase.
	\item Verdaderos Negativos (VN): Elementos clasificados correctamente que no pertenecen a la clase.
	\item Falsos Positivos (FP): Elementos clasificados incorrectamente que pertenecen a la clase.
	\item Falsos Negativos (FN): Elementos clasificados incorrectamente que no pertenecen a la clase.
\end{itemize}

TODO FOTO DE MATRIZ DE CONFUSION BINARIA

La precisión es la medida que indica la probabilidad de que la clasificación de una clase sea correcta. Esto 
se puede observar como la proporción de los elementos correctamente clasificados sobre el total de 
elementos clasificados:

\begin{equation}
	prec_i = \frac{VP}{VP + FP}
\end{equation}.

En problemas de clasificación múltiple surge la versión macro de esta medida calculada como la media de todas
las precisiones de las clases existentes:

\begin{equation}
	prec_{macro} = \sum^K_{i=1} \frac{prec_i}{K}
\end{equation}.

El recobrado es la medida que indica la probabilidad de que se clasifique correctamente un elemento de la clase
del total existente. Esto se puede observar como la proporción de los elementos correctamente clasificados sobre el 
total de elementos que pertenecen a la clase:

\begin{equation}
	rec_i = \frac{VP}{VP + FN}
\end{equation}.

En problemas de clasificación múltiple surge la versión macro de esta medida calculada como la media de todos
los recobrados de las clases existentes:

\begin{equation}
	rec_{macro} = \sum^K_{i=1} \frac{rec_i}{K}
\end{equation}.

La medida F1 es la media armónica de la precisión y el recobrado. En esta la contribución de la precisión y el
recobrado al resultado final es el mismo, aunque es posible buscar variaciones de acuerdo a al problema a tratar:

\begin{equation}
	f1_i = 2 (\frac{prec_i · rec_i}{prec_i + rec_i})
\end{equation}.

En problemas de clasificación múltiple surge la versión macro de esta medida calculada la propia medida F1 pero
utilizando la precisión y recobrado macro del problema:

\begin{equation}
	f1_{macro} = 2 (\frac{prec_{macro} · rec_{macro}}{prec_{macro} + rec_{macro}})
\end{equation}.

Las m'etricas anteriores est'an acotadas por los valores 0 y 1 donde 1 representa la mejor evaluación y 0 la 
peor.

La entropía cruzada se encarga de evaluar qué tan diferentes son dos funciones de distribución $p$ y $q$, su 
resultado es un n'umero no negativo que a medida que sean m'as pequeños los valores indican mayor similitud. 
En su versión discreta se formula:

\begin{equation}
	H(p, q) = - \sum_{x \in D} p(x) \log q(x)
\end{equation}

\subsubsection{Curvas de aprendizaje}

Es necesario además de evaluar el resultado final del modelo, evaluar el proceso de entrenamiento. En esta etapa 
se pueden diagnosticar varias deficiencias en este proceso. Para un correcto diagnóstico se divide el conjunto de 
datos en tres partes:

\begin{itemize}
	\item \textbf{entrenamiento}: Utilizada para el entrenamiento del modelo.
	\item \textbf{validación}: Utilizada para evaluar el desempeño del modelo en entrenamiento.
	\item \textbf{prueba}: Utilizada para evaluar el resultado final.
\end{itemize}

Las curvas de aprendizaje constituye la principal herramienta para evaluar el proceso de aprendizaje.
Estas están formadas por las mediciones de métricas a lo largo del entrenamiento calculadas a partir de 
los conjuntos de validación y entrenamiento. La linea correspondiente al conjunto de entrenamiento cuantifica 
el aprendizaje del modelo o también el error de entrenamiento y la correspondiente a la de validación cuantifica 
la generalización o el error de generalización. Existen tres comportamientos escenciales:

\begin{itemize}
	\item Bajo ajuste (\emph{underfitting} en inglés).
	\item Sobreajuste (\emph{overfitting} en inglés).
	\item Buen ajuste.
\end{itemize}

El bajo ajuste ocurre cuando el modelo no es capaz de aprender del conjunto de datos o cuando este aún puede aprender 
más. Las curvas de aprendizaje en estos casos se caracterizan por ser una linea plana o valores ruidosos con alta pérdida.

TODO FOTO de LO DICHO ANTERIOR. Guiarse por el libro [brownlee2018better]

Entre las formas más sencillas de combatir el bajo ajuste de los modelos consiste en complejizar el modelo, al añadir
capas o aumentar las dimensiones se este aumenta la expresividad de este y por lo tanto su ajuste. Si este método 
no funciona es posible considerar un cambio de arquitectura hacia una que pueda extraer más información de la 
estructura de los datos. 

El sobreajuste es el fenómeno en el que el modelo se aprende los datos de entrenamiento extremadamente bien, incluso
el ruido en estos datos, esto trae consigo que falla en generalizar el problema para nuevas entradas. Las curvas 
características de este fenómeno presentan una divergencia en los errores de entrenamiento y validación a medida
que se entrena el modelo, mientras que la de entrenamiento mejora la de validación tiende a empeorar. 

TODO FOTO de OVERFITTING. Guiarse por el libro [brownlee2018better]

Existen varios métodos para combatir el sobreajuste, uno sencillo es simplificar el modelo quitándole capas 
o disminuyendo sus dimensiones. Además de esto existen regularizaciones que se pueden aplicar para evitar que 
las capas dependan exclusivamente de pocos atributos, entre esta familia los más usados son la regularización
L1 y L2 las cuales se definen como la suma del valor absoluto de los atributos y la suma del cuadrado de sus 
atributos respectivamente. Otra medida para prevenir el sobreajuste es el agrego de capas de abandono 
(\emph{dropout} en inglés), estas capas desactivan neuronas de la arquitectura y obligando a 
estas a ser robustas y depender del comportamiento de la población, en lugar de la actividad de otras unidades 
específicas [\cite{baldi2013dropout}]. La terminación temprana (\emph{early stopping} en inglés) del entrenamiento
se utiliza para parar este en el momento en que el error de generalización comienza a subir, impidiendo así que 
se sobreentrene el modelo.

Finalmente un buen ajuste es el resultado que se alcanza cuando tanto la curva de validación como de entrenamiento
presentan valores pequeños y similares, consecuentes con una correcto aprendizaje y generalización.

TODO FOTO de Good Fit. Guiarse por el libro [brownlee2018better]

Otro problema observable a partir del análisis de las curvas de aprendizaje constituye la detección de conjuntos
de datos no representativos. Un conjunto de datos no representativo es uno que puede no 
capturar las características estadísticas relativas a otro conjunto de datos extraído del mismo dominio.
Esto puede pasar que los conjuntos de entrenamiento o de validación. En caso del conjunto de entrenamiento
se puede identificar si la pérdida en el conjunto de entrnamiento conlleva a una ganancia en el conjunto de 
validación y viceversa quedando al final con una separación entre ambos valores. En el caso del conjunto de 
validación se presenta como una curva ruidosa, también se puede dar el caso de que el conjunto  de validación
sea más fácil de predecir que el de entrenamiento, en este caso se observa como la curva de validación permanece
siempre por debajo de la de entrenamiento.

TODO FOTO de Unrepresentative Train Data, Unrepresentative Dev Data (Noise, Easier than train). Guiarse por el libro [brownlee2018better]

Para combatir estos problemas se puede aumentar la cantidad de elementos en los conjuntos de entrenamiento o 
validación en dependencia de donde ocurra.

\subsection{Aumento de datos}

El aumento de datos consiste en acciones para aumentar la diversidad de un conjunto de datos sin recolectar
nuevos datos explícitamente [\cite{feng2021data}]. En datos continuos, como imágenes o valores numéricos el 
aumento de datos puede ser realizado al añadirle perturbaciones a entradas existentes, en caso de las imágenes 
técnicas como el volteado (\emph{flipping}) o recortado (\emph{cropping}) son usadas. Los textos son un tipo 
de datos discreto y por lo tanto las técnicas anteriores no pueden ser aplicadas directamente. Para PLN
se han estudiado diversas técnicas para el aumento de datos, una de estas consiste en el cambio del árbol de 
dependencia de la oración mediante operaciones de intercambio y borrado de nodos [TODO \cite{}], también se han utilizado 
gaceteras para el intercambio de palabras por sinónimos [TODO \cite{}], la traducción de textos hacia un lenguaje y luego 
de vuelta al lenguaje origen es otro ejemplo o \emph{backtranslation} en inglés. 

\subsection{Aprendizaje Conjunto}

El aprendizaje conjunto (\emph{ensemble learning} en inglés) constituyen técnicas encaminadas al aprovechamiento
de soluciones encontradas por diferentes modelos combinándolas y mejorándolas para encontrar una mejor solución 
al problema. Estos métodos son efectivos en la reducción de la varianza y el bias de los resultados obteniendo así
mejores resultados [\cite{dietterich2002ensemble}] 

Las manera TODO

\subsection{Métodos de optimización}

% Optimización, encontrar la solucion exacta es un problema dificil, se usan metodos para buscar soluciones aproximadas.
% Descenso por gradiente Algoritmo general, Optimizadores (Adams, Stocastic Gradient Descent, Exponential Decay)

El objetivo de AA es encontrar los extremos de una función de costo, este proceso es una tarea 
desafiante ya que la gran mayoría de estas funciones no son convexas y por lo tanto no existe un algoritmo
que asegure la convergencia hacia un extremo global. Para resolver este problema exiten múltiples heurísticas,
la más usasda es el descenso por gradiente, este algoritmo contiene múltiples variaciones. La idea básica consiste 
en el cálculo del vector gradiente de la función de error $e$ con respecto a los parámetros del modelo $x$ y una vez se 
tiene dicho vector se evalua en la asignación actual de los parámetros $x_i$ y se realiza un corrimiento de este punto 
en contra del gradiente para disminuir el error.

\begin{equation}
	x_{i+1} = x_i - \alpha \nabla f(x_i)
\end{equation}\label{eq:gradien_descent}

En la ecuación anterior $\alpha$ se le conoce como tasa de aprendizaje (\emph{learning rate} en inglés),
este valor cuantifica cuánto se toma del vector del gradiente para actualizar los parámetros, esto 
se puede ver como el aprendizaje del modelo.

Variantes de este algoritmo eficientes para el proceso de entrenamiento de modelos de AA han sido 
creadas. Entre las técnicas utilizadas se encuentran Descenso por Gradiente Estocástico (\textbf{SGD} por sus siglas 
en inglés \emph{Stochastic Gradient Descent}) [TODO \cite{}], tasa de aprendizaje dinámica con sus diferentes
variantes (exponencial, polinómica) [TODO \cite{}], RMSProp [TODO \cite{}] y Adam [TODO \cite{}]. 

\section{Extracción de Argumentos}

Varias investigaciones y propuestas han salido para dar respuesta a los problemas asociados a EA, mostrando
una variedad en enfoques y métodos.

En [\cite{palau2009argumentation}] se propone
el uso de modelos estadísticos como \emph{Naive Bayes} (\textbf{NB}) y \emph{Support Vector Machine} (\textbf{SVM}) 
para la clasificación de 
oraciones en argumentativas o no y en su rol argumentativo en caso de que sea argumentativa, en este
caso se asume que las componentes argumentativas son oraciones completas. Para la predicción de relaciones
se usa un enfoque basados en reglas con la creación de una Gramática Libre de Contexto. Las representaciones
de las oraciones se ven dadas por atributos creados a mano, dado el conocimiento experto sobre la argumentación
en el tema tratado, elementos como adverbios, verbos, signos de puntuación, palabras clave, estadísticas del texto
(Tamaño de oración, distancia media de palabras) son usados para la extracción y clasificación de las UDA, además
se usan también como base en la creación de las relgas de la gramática para la extracción de relaciones.

[\cite{goudas2015argument}] al igual que [\cite{palau2009argumentation}] clasifica las oraciones como
argumentativas o no mediante el uso de diferentes clasificadores como \textbf{NB}, \emph{Random Forest}, Regresión
Logística y \textbf{SVM}. En este trabajo se aumenta la grandularidad de la segmentación al permitir
la extracción de los segmentos que contienen la carga argumentativa de dentro de las oraciones previamente clasificadas
como tal, esto se realiza mediante la extracción de etiquetas BIO de las oraciones con el uso de un 
\textbf{CRF}. La predicción de las relaciones es modelado como un problema de clasificación
usando \textbf{SVM} para clasificar pares de UDA en relacionados o no. Atributos creados a mano 
son usados en la extracción de UDA entre estos están posición de la oración en el texto, cantidad de verbos, comas, adverbios,
palabras, entidades en la oración, también se emplean gaceteras que guardan entidades relacionadas con el dominio 
específico y palabras clave indicadoras de frases argumentativas. 

[\cite{stab2017parsing}] propone un mecanismo de segmentación basado en \textbf{CRF}. La clasificación
y predicción de relaciones es modelado conjuntamente como con dos clasificadores \textbf{SVM} y un problema
de Optimización Lineal Entero que encuentra la mejor estructura y asegurar una disposición arborea. En la segmentación
de las UDA se extraen por cada token su posición en el texto, si precede o sucede a un signo de puntuación, su parte de
la oración, la probabilidad de que sea el comienzo de una UDA dado sus tokens anteriores, entre otros. Para la extracción
y clasificación de relaciones se proponen otros conjuntos de atributos como la cantidad de sustantivos comunes entre
las componentes fuente y el objetivo, la presencia de indicadores argumentativos, representaciones vectoriales de tokens.

En [\cite{eger2017neural}] se enfocaron en presentar el problema de EA como uno \emph{end-to-end}. 
Para esto presentaron varias propuestas, entre ellas se encontraba
modelar el problema como un uno de secuencia a secuencia, usando \textbf{RNN} como 
\textbf{LSTM} en versiones bidireccionales capturando información desde ambos lados de la secuencia,
para la representación de las palabras se extrajo información morfológica de las palabras mediante Redes Neuronales Convolucionales,
al final realizan la clasificación de la secuencia con un \textbf{CRF}. 
Realizaron experimentos al modelar el problema como uno de \emph{Dependency Parsing} en [\cite{kiperwasser2016simple}]. Este problema
consiste en construir un árbol de dependencia que codifique las estructuras argumentativas, en este problema 
se tiene que decidir entre varias opciones (\emph{shift}, \emph{reduce}) en dependencia del contenido de la pila y en el buffer.
El problema fue modelado también como un problema de reconocimiento de entidades nombradas, en donde las entidades son las UDA.

[\cite{galassi2018argumentative}] propone el uso de redes residuales y en combinación con mecanismos de atención
para la creación de un modelo el cual, conjuntamente, clasifica el tipo de UDA y la relación existentes entre estas.
Este trabajo define el concepto de distancia argumentativa, añadiéndolo como feature y asume que las UDA ya fueron 
extraídas. En este caso además de la distancia argumentativa las secuencias son representadas por sus representaciones
vectoriales de \textbf{GloVe}. Este trabajo asume la previa extracción de las UDAs.

En [\cite{dykes2020reconstructing}] se propone métodos basados en reglas para la extracción de argumentos sobre
textos en Twitter, estos métodos se centran en la confección de reglas basadas en anotaciones lingüísticas como
partes de la oración, lemas de palabras. La recuperación está basada en los esquemas argumentativos comunes presentes
en los textos. Este método apunta a una mayor precisión prescindiendo de recobrado. Dada las reglas creadas y el tipo
de datos con que se trabaja, cadenas de texto pequeñas, estos algoritmos tienden a tener una alta precisión aunque 
bajo recobrado, esto en conjuntos de datos grandes no es un gran problema, pero en conjuntos de menor tamaño o estructura 
más compleja pierden efectividad.

En resumen se contempan disímiles enfoques al problema de EA desde una perspectiva enmarcada en modelos 
simbólicos, estadísticos y neuronales en versiones tanto secuenciales como \emph{end-to-end}. 
Cada uno de estos modelos presentan sus ventajas y desventajas a la hora de contruirlos, 
extenderlos y comprender su funcionamiento. En modelos simbólicos se presenta una alta
precisión en dominios específicos debido a que estos se construyen teniendo en cuenta reglas específicas a un
contexto dado. Estos modelos son poco escalables y difíciles de mantener ya que son sus reglas son construídas
a mano y dicho proceso requiere de conocimiento experto y tiempo. Modelos estadísticos son
característicos de usar conjuntos de atributos creados a mano, dichos atributos son difíciles
de encontrar, calcular y pueden no poseer relevancia en otros contextos diferentes a los que fueron creados. 
Además la necesidad de conocimiento experto es necesaria para su confección. Los modelos neuronales poseen
un mayor poder de adaptabilidad, en estos la entrada es codificada en una representación que es aprendida por
el mismo algoritmo, permitiendo seguir viable para distintos esquemas argumentativos. Los modelos simbólicos y 
estadísticos poseen la ventaja de poder explicar el porqué de los resultados devueltos cosa que se vuelve casi
imposible en modelos neuronales.

Dado que la EA es un proceso en el cual se necesita pasar por varias tareas, estas deben de ser completadas
de alguna forma. Una manera de completarlas es hacerla una a la vez, independiente una de otra y pasándole
la salida de etapas anteriores a las etapas siguientes. Esta manera secuencial de realizar las 
tareas es bastante simple y ayuda a la creación de modelos simples y con tareas bien definidas, aunque trae consigo 
la propagación de los errores a través del proceso y el no aprevechamiento de las interrelaciones entre variables 
computadas de procesos anteriores. Modelos \emph{end-to-end} en cambio poseen la habilidad de modelar el problema 
desde su inicio hasta su final de manera conjunta, mediante \emph{Multi-Task Learning} (\textbf{MTL}) se modelan
las tareas de manera conjunta creando modelos complejos aunque con una propagación de error menor.

\section{Proyección de corpus}

La EA no es un campo de PLN que presenta una gran cantidad de datos anotados con los cuales se pueda realizar 
un entrenamiento, además de esto la gran mayoría de corpus existentes se encuentran en lenguajes como inglés o alemán,
haciendo difícil el desarrollo de esta rama en otros lenguajes.
La escasez de estos datos es en gran parte debida al elevado costo monetario, de tiempo y de recursos humanos que se utiliza
en su creación. En orden de poder desarrollar la EA en otros lenguajes, como el español, se han investigado diferentes vertientes
para la construcción de conjuntos de datos en estos lenguajes de pocos recursos a partir de los conjuntos de datos ya 
existentes.

La proyección de etiquetas consiste en un algoritmo en donde se 
transfieren las etiquetas de un corpus anotado a nivel de tokens en un lenguaje origen hacia su traducción en un 
lenguaje objetivo. Transferencia directa se refiere a (TODO completar)
En [\cite{eger2018cross}] se muestra que la proyección constituye una mejor alternativa sobre la transferencia
directa y presentan un algoritmo para realizar la proyección de las etiquetas dadas las alineaciones de palabras.
El proceso se divide en varias partes:

\begin{enumerate}
	\item Traducción de oraciones.
	\item Alineación de palabras.
	\item Proyección de etiquetas.
\end{enumerate}

\subsection{Traducción de oraciones}

La Traducción Automática consiste en el proceso de usar inteligencia artificial para
traducir texto de un lenguaje fuente a un lenguaje objetivo sin la intervención humana.
En la actualidad este campo ha dado un gran paso pasando de modelos estadísticos a modelos
neuronales llevando a tener traducciones de una alta calidad sin variar significativamente de la humana, 
condición necesaria para una buena proyección [\cite{eger2018cross}].

Por lo anterior es posible la traducción de corpus hacia otros lenguajes mediante las
herramientas existentes obteniendo una buena representación final, la cual se puede usar en la creación de
oraciones alineadas, primer paso para la proyección de etiquetas.

\subsection{Alineación de palabras}

La alineación de palabras consiste en encontrar las palabras generadas en el lenguaje objetivo por las 
palabras en el lenguaje fuente.
Este problema es atacado en los modelos IBM, modelos estadísticos de traducción automática que usan estos 
índices para obtener más información. Estos modelos son principalmente Bayesianos y han sido la base de
otras herramientas como FastAlign [\cite{dyer2013fastalign}] que incorpora una reducción sustancial del
tiempo de cómputo y además de una mejora al modelo, EFMARAL [\cite{ostling2016efficient}] que usa
Cadenas de Markov Monte Carlo. Modelos más recientes se han enfocado en explotar las representaciones
vectoriales de palabras y el uso de métodos de atención para la extracción de las
alineaciones [\cite{dou2021word}].

FOTO de EJEMPLO de ALINEACION

\subsection{Proyección de etiquetas}

La proyección de etiquetas consiste en transportar las etiquetas de las palabras en la secuencia origen
hacia las palabras de la secuencia destino tomando como datos las alineaciones entre estas. En [\cite{yarowsky2001inducing}]
se trata el problema de proyección de frases nominales, estas frases tienen como característica que son resistentes
a ser divididas en caso de ser traducidas, dicha propiedad se cumple para las UDA también, que aunque evidencian 
cambios en el orden de las palabras, mantienen la misma ventana. La proyección de UDA son más simples en el caso
que solamente se tiene en cuenta la ventana y las etiquetas en estas son constantes, no pasa con la proyección en
frases nominales las cuales pueden cambiar dentro de una ventana, por lo que algoritmos más simples existen
para esta tarea [\cite{eger2018cross}].
